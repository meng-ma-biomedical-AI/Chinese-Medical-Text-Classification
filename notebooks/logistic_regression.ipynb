{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练逻辑回归(LR)模型，进行医学文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 获得训练数据，测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "# 这是训练数据\n",
    "train_file = \"../data/train.data\" \n",
    "train_data = pd.read_csv(train_file, sep=\"\\t\", header=None, names=[\"id\", \"category\", \"sentence\"])\n",
    "\n",
    "# 这是测试数据\n",
    "test_file = \"../data/test.data\" \n",
    "test_data = pd.read_csv(test_file, sep=\"\\t\", header=None, names=[\"id\", \"sentence\"])\n",
    "print(\"训练集数据有：{}条\".format(len(train_data)))\n",
    "print(\"测试集数据有：{}条\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示前10条训练数据\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练数据的类别，及其数目\n",
    "train_data.groupby(['category'],  as_index=False)['category'].agg({'count': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示前10条测试数据\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据格式转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 把文本转换为向量，使用TF-IDF算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本转换为向量\n",
    "def tfidf(raw_text):\n",
    "    #将中文文本分词，并以空格隔开，使用jieba分词工具。\n",
    "    import jieba\n",
    "    preprocess_setences = []\n",
    "    sentences =raw_text\n",
    "    for sentence in sentences:\n",
    "        words = [word for word in jieba.cut(sentence)]\n",
    "        preprocess_setences.append(' '.join(words))\n",
    "\n",
    "    # 使用sklearn自带的CoutVectorizer模块即可简单生成特征向量\n",
    "    from sklearn.externals import joblib\n",
    "    from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "    \n",
    "    # 词向量的维度设定为1000，可根据情况设定，少则文本信息不足，多则增加训练时间\n",
    "    counter = CountVectorizer(max_features=200)  \n",
    "    counts = counter.fit_transform(preprocess_setences)\n",
    "    print('countvectorizer词表:\\n',counter.vocabulary_)  \n",
    "    \n",
    "    # 将词汇表输出到文件\n",
    "    with open(\"LR_count_voca\", \"w\") as f:  \n",
    "        for word, freq in counter.vocabulary_.items():\n",
    "            f.write(\"{}\\t{}\\n\".format(word, freq))\n",
    "    # print('词向量矩阵:\\n',counts.toarray())  #fit_transform后查看具体向量\n",
    "    \n",
    "    tfidfer = TfidfTransformer()\n",
    "    tfidf = tfidfer.fit_transform(counts)\n",
    "    print('tfidf向量矩阵：\\n',tfidf.toarray())  #fit_transform后查看具体向量矩阵\n",
    "    joblib.dump(counter, 'LR_count_vect') #保存矢量化\n",
    "    return list(tfidf.toarray())\n",
    "\n",
    "train_text = list(train_data['sentence'])\n",
    "result = tfidf(train_text)\n",
    "# 将转化后的向量保存在data_X中\n",
    "data_X = [np.array(i) for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词云显示所选的组成词向量的词\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "cloud_words = []\n",
    "with open(\"LR_count_voca\", \"r\") as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        cloud_words.extend([l[0]] * int(l[1]))\n",
    "wordcloud_ = WordCloud(background_color=\"white\", collocations=False, max_words=1000, font_path=\"C:\\\\Windows\\\\Fonts\\\\msyh.ttc\").generate(\" \".join(cloud_words))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud_, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 将类别(15个)转换为数字(0-14)表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将raw_category的15个类别转换为0-14的数字\n",
    "raw_category = list(train_data['category'])\n",
    "category_ = sorted(list(set(list(train_data['category']))))\n",
    "category_index = {}\n",
    "for i in category_:\n",
    "    category_index[i] = category_.index(i)\n",
    "    print(category_.index(i), i)\n",
    "data_Y = np.array([category_index[c] for c in raw_category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 查看转换后的训练数据的输入与输出格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_X[1]) #应为太大，8000条数据，只显示第一行查看一下\n",
    "print(data_Y) # 所有的类别，8000条"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练逻辑回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 使用训练数据训练，并保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# 训练\n",
    "start_time = time.time()\n",
    "model = LogisticRegression(penalty='l2')\n",
    "model.fit(data_X, data_Y)\n",
    "print('training took %fs!' % (time.time() - start_time))\n",
    "\n",
    "# 保存模型\n",
    "model_save_file = \"LR.mkl\"\n",
    "pickle.dump(model, open(model_save_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 如何预测新输入？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 构建预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "tags = (\"Addictive Behavior\", \"Age\", \"Allergy Intolerance\", \"Compliance with Protocol\", \"Consent\", \"Diagnostic\", \"Disease\", \"Enrollment in other studies\", \"Laboratory Examinations\", \"Life Expectancy\", \"Organ or Tissue Status\", \"Pharmaceutical Substance or Drug\", \"Risk Assessment\", \"Smoking Status\", \"Therapy or Surgery\")\n",
    "def predict(sentence):\n",
    "    # 利用之前的词汇构建特征向量\n",
    "    words = [word for word in jieba.cut(sentence)]\n",
    "    preprocess_s = ' '.join(words)\n",
    "\n",
    "    # 导入tf-idf词表\n",
    "    count_vect = joblib.load('LR_count_vect')\n",
    "    tfidfer = TfidfTransformer()\n",
    "    X_new_counts = count_vect.transform([preprocess_s])\n",
    "    tfidf = tfidfer.fit_transform(X_new_counts)\n",
    "    criteria_X = tfidf.toarray()\n",
    "\n",
    "    # 使用之前训练的参数，进行预测\n",
    "    predict_inputs = model.predict(criteria_X)\n",
    "    predict_results = tags[predict_inputs[0]]\n",
    "    return predict_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 输入一条句子，进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入一条新句子\n",
    "# s = \"3.有糖尿病的患者。\"\n",
    "# s = \"1.术后发生非计划再次手术；\"\n",
    "s = \"白细胞计数升高或大于10^9/L\"\n",
    "predict(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 输入一份文件，进行预测，并保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测测试集数据的结果，并保存为文件test.predict\n",
    "import codecs\n",
    "test_sentences = list(test_data[\"sentence\"])\n",
    "n = 0\n",
    "with codecs.open(\"../data/test.LR.predict\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in test_sentences:\n",
    "        n += 1\n",
    "        c = predict(s) # 预测\n",
    "        f.write(\"s{}\\t{}\\t{}\\n\".format(n,c,s)) # 保存\n",
    "        # 显示进度\n",
    "        if n%10 ==0: print(\"[INFO] Processing: {0}/{1} \\t {2:<50s} {3:10s}\".format(str(n), str(len(test_sentences)), c, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 系统评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "import evaluation\n",
    "results = evaluation.Record_results('../data/test.gold', '../data/test.LR.predict')\n",
    "evaluation = evaluation.Evaluation(results.records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
